{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caiogasparine/3253-Machine-Learning/blob/main/W3_Homework_Caio_Gasparine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os8UEIgdR24Y"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(123)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['axes.labelsize'] = 14\n",
        "plt.rcParams['xtick.labelsize'] = 12\n",
        "plt.rcParams['ytick.labelsize'] = 12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0z7V3gOR24c"
      },
      "source": [
        "### Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1G_cA64R24d"
      },
      "source": [
        "Q1. Build a classification model for the default of credit card clients dataset. More info here:\n",
        "https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n",
        "\n",
        "- Explore the data\n",
        "- Make sure you build a full data pipeline\n",
        "- Do you require any data pre-processing? Are all the features useful? (Use only raw features)\n",
        "- set the random seed to 123 (For splitting or any other random algorithm)\n",
        "- Split data into training (80%) and testing (20%)\n",
        "- Follow similar procedure as the one for week 2 (End-to-end Machine Learning Project). Remember apendix B\n",
        "- Study the ROC Curve, decide threshold\n",
        "- Use 2 classifiers.\n",
        "    - Random Forest\n",
        "        - tune only: n_estimators: {3, 4, 6, 7, 10, 20, 50, 100} \n",
        "    - KNN Classfier \n",
        "        - tune only: n_neighbors: {3, 4, 5, 7, 10, 20, 50} \n",
        "    - Which one performs better in the cross validation?\n",
        "    \n",
        "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "\n",
        "http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
        "\n",
        "- Cross-validation with 4-folds.\n",
        "\n",
        "- Other paramenters -> Use default\n",
        "\n",
        "Notes:\n",
        "  - Make your code modular, the second part of the assignmet you will have to repeat. \n",
        "  - Include documentation for your code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcK9rr6lR24d"
      },
      "source": [
        "### Caio-Gasparine-2023-01-30\n",
        "\n",
        "import os\n",
        "from six.moves import urllib \n",
        "\n",
        "DOWNLOAD_PATH = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/\"\n",
        "CREDIT_PATH = os.path.join(\"datasets\", \"credit-default\")\n",
        "CREDIT_URL = DOWNLOAD_PATH + \"default%20of%20credit%20card%29clients.xls\"\n",
        "\n",
        "#CREDIT URL // DOWNLOADING THE DATASET\n",
        "def fetch_creditdefault_data(credit_url=CREDIT_URL, credit_path=CREDIT_URL):\n",
        "  if not os.path.isdir(credit_path):\n",
        "    os.makedirs(credit_path)\n",
        "    file_path = os.path.join(credit_path, \"default of credit card clients.xls\")\n",
        "\n",
        "    urllib.request.urlretrieve(credit_url, file_path)\n",
        "\n",
        "  fetch_creditdefault_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "def load_CRDEFAULT_data(credit_path=CREDIT_PATH):\n",
        "  file_path = os.path.join(credit_path, \"default of credit card clients.xls\")\n",
        "  return pd.read_excel(file_path, sheet=0, skiprows=1, header=0)\n",
        "  pd.read_excel(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\" , skiprows=1, header=0)\n",
        "\n",
        "df = pd.read_excel(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\" , skiprows=1, header=0)\n"
      ],
      "metadata": {
        "id": "OuJmEmiDZfVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PART 1 - EXPLORE THE DATA\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "df.hist(bins=50, figsize=(20,15))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1rkSD_POU208"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17orkXFOXUe6"
      },
      "source": [
        "df = df.drop(\"ID\", axis=1)\n",
        "TARGET = \"default payment next month\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### CONTINUE HERE -> VIDEO AT 7:15 \n",
        "##### Creating Training dataset and Test using stratified sample on Defaulting payment next month \n",
        "#####\n",
        "columns_predictors = [col for col in df.columns if col not in [TARGET]]\n",
        "columns_categorical = [\"SEX\" , \"EDUCATION\", \"MARRIAGE\", \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", 'PAY_5', 'PAY_6']\n",
        "columns_numerical = [col for col in columns_predictors if col not in columns_categorical]\n",
        "\n",
        "print(f\"TARGET : {TARGET}\")\n",
        "print(f\"columns_predictors: {columns_predictors}\")\n",
        "print(f\"columns_categorical: {columns_categorical}\")\n",
        "print(f\"columns_numerical: {columns_numerical}\")\n"
      ],
      "metadata": {
        "id": "MfSSZlWHdAzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Define X and y\n",
        "#### \n",
        "X = df[columns_predictors]\n",
        "y = df[TARGET]\n"
      ],
      "metadata": {
        "id": "TayTxAZpz3co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Train / Test Split\n",
        "#### Stratified sampling based on the target + Randomized sample for X and y\n",
        "#### You dont need to specify the indexes\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10086, stratify=y)\n",
        "\n",
        "print(f\"X_train.shape: {X_train.shape}\")\n",
        "print(f\"X_test.shape: {X_test.shape}\")\n",
        "print(f\"y_train.shape: {y_train.shape}\")\n",
        "print(f\"y_test.shape: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "dwS1vkF50TjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Build Data Pipeline\n",
        "#### VIDEO AT 18:08\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer \n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
        "\n",
        "#### convert the non tranformed data into a list\n",
        "columns_categorical_list = list(columns_categorical)\n",
        "columns_numerical_list = list(columns_numerical)\n",
        "\n",
        "pipeline_categorical = Pipeline([(\"onehot\", OneHotEncoder(drop=\"first\"))])\n",
        "pipeline_numerical = Pipeline([(\"scaler\", StandardScaler())])\n",
        "pileline_full = ColumnTransformer([\n",
        "    ('categorical', pipeline_categorical, columns_categorical_list),\n",
        "    ('numerical', pipeline_numerical, columns_numerical_list),\n",
        "])"
      ],
      "metadata": {
        "id": "NsdUj8_h07Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Using Column Transformer ro merge - categorical + numerical features\n",
        "pipeline_full = ColumnTransformer([\n",
        "    ('categorical',pipeline_categorical,columns_categorical),\n",
        "    ('numerical', pipeline_numerical, columns_numerical),\n",
        "])\n",
        "pipeline_full.fit(X_train)\n",
        "X_train_transformed = pipeline_full.transform(X_train)\n",
        "X_test_transformed = pipeline_full.transform(X_test)\n",
        "print(f\"X_train_transformed.shape: {X_train_transformed.shape}\")\n",
        "print(f\"X_test_transformed.shape: {X_test_transformed.shape}\")\n",
        "\n",
        "####increased to 82 columns"
      ],
      "metadata": {
        "id": "qiCfahtZ58EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_transformed\n"
      ],
      "metadata": {
        "id": "PxMFTMXf7Sjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Get the feature names for the Sparsed Matrix\n",
        "#### Video at 21:00\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "import sklearn\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aL4mTfWy7j8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df.loc[df.index[3329],\"BILL_AMT6\"]\n",
        "df.iloc[3330]"
      ],
      "metadata": {
        "id": "Zw5R35B-cehl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGs1LQQ3_lqF"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMkflm9Ay-UV"
      },
      "source": [
        "max(df.loc[:,\"PAY_0\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yghzvq0VR24i"
      },
      "source": [
        "# Cross-validation with 5-folds\n",
        "# Did you get different results compared to the 4-fold case?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3HDtUyzR24j"
      },
      "source": [
        "#### Conclusions?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzyM49VVR24k"
      },
      "source": [
        "Explain your results and choices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZNssFNgR24l"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-6LCux9R24o"
      },
      "source": [
        "Q2. (Optional) Write a function that can shift an MNIST image in any direction (left, right, up, or down) by one pixel. Then, for each image in the training set, create four shifted copies (one per direction) and add them to the training set. Finally, train your best model on this expanded training set and measure its accuracy on the test set. You should observe that your model performs even better now! This technique of artificially growing the training set is called data augmentation or training set expansion. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5UGBa2AR24o"
      },
      "source": [
        "### Conclusions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBBG-JBHR24o"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}